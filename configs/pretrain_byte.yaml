defaults:
  - _self_
  # - model: small

ngpus: 1  # Multi-GPU training
tokens: 259  # 256 bytes + PAD + BOS + EOS

training:
  batch_size: 256  # Larger batch for pre-training
  accum: 1
  n_iters: 20000  # Pre-training iterations
  snapshot_freq: 5000
  log_freq: 100
  eval_freq: 1000
  snapshot_freq_for_preemption: 2500
  weight: standard
  snapshot_sampling: True
  ema: 0.999
  disable_checkpoint_loading: True  # Set to true to start from scratch
  max_samples: 10000000

data:
  train: byte_wikipedia
  valid: byte_wikipedia
  cache_dir: data

graph:
  type: uniform
  file: data
  report_all: False

noise:
  type: loglinear
  sigma_min: 1e-4
  sigma_max: 20

sampling:
  predictor: analytic
  steps: 64
  noise_removal: True

eval:
  batch_size: 64
  perplexity: True
  perplexity_batch_size: 16

optim:
  weight_decay: 0.01
  optimizer: AdamW
  lr: 3e-4
  beta1: 0.9
  beta2: 0.95
  eps: 1e-8
  warmup: 2000
  grad_clip: 1.
  # Muon optimizer settings
  use_muon: False  # Set to true to use Muon optimizer
  muon_lr: 0.002  # Learning rate for Muon (hidden weights)
  muon_momentum: 0.95
  muon_nesterov: True
  muon_ns_steps: 5
  adamw_lr: 3e-4  # Learning rate for AdamW (embeddings, heads, biases)

model:
  hidden_size: 768
  cond_dim: 96
  length: 32
  n_blocks: 12
  n_heads: 12
  scale_by_sigma: False
  dropout: 0.1

logging:
  use_wandb: ${oc.env:USE_WANDB,false}  # Set USE_WANDB=true to enable
  # use_wandb: False
  wandb_project: "palindrome-diffusion"
  wandb_name: "larger_adamW_32_length_uniform"
  wandb_entity: null  # Your wandb entity/username
  log_grad_norm: true
  log_param_norm: true
  log_learning_rate: true

hydra:
  run:
    dir: exp_local/pretrain_byte/${now:%Y.%m.%d}/${now:%H%M%S}
  sweep:
    dir: exp/pretrain_byte/${now:%Y.%m.%d}/${now:%H%M%S}
    subdir: ${hydra.job.num}
