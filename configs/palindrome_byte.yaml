defaults:
  - _self_
  - model: small

ngpus: 4  # Multi-GPU training
tokens: 259  # 256 bytes + PAD + BOS + EOS

training:
  batch_size: 64  # Smaller batch size for byte-level
  accum: 1
  n_iters: 10000
  snapshot_freq: 1000
  log_freq: 50
  eval_freq: 500
  snapshot_freq_for_preemption: 1000
  weight: standard
  snapshot_sampling: True
  ema: 0.999  # Faster EMA for quicker adaptation
  pretrain_checkpoint: null  # Path to pre-trained checkpoint
  disable_checkpoint_loading: false  # Set to true to start from scratch

data:
  train: byte_palindrome
  valid: byte_palindrome
  cache_dir: data

graph:
  type: absorb
  file: data
  report_all: False

noise:
  type: loglinear
  sigma_min: 1e-4
  sigma_max: 20

sampling:
  predictor: analytic
  steps: 64  # Fewer steps for faster sampling
  noise_removal: True

eval:
  batch_size: 32
  perplexity: True
  perplexity_batch_size: 16

optim:
  weight_decay: 0.01
  optimizer: AdamW
  lr: 3e-4  # Increase learning rate
  beta1: 0.9
  beta2: 0.95  # Adjust beta2 for better training
  eps: 1e-8
  warmup: 2000  # Longer warmup
  grad_clip: 1.

model:
  hidden_size: 384  # Smaller model for byte-level
  cond_dim: 96
  length: 128  # Max sequence length for palindromes
  n_blocks: 6   # Fewer blocks
  n_heads: 6    # Fewer heads
  scale_by_sigma: True
  dropout: 0.1

logging:
  use_wandb: ${oc.env:USE_WANDB,false}  # Set USE_WANDB=true to enable
  wandb_project: "palindrome-diffusion"
  wandb_entity: null  # Your wandb entity/username
  log_grad_norm: true
  log_param_norm: true
  log_learning_rate: true

hydra:
  run:
    dir: exp_local/palindrome_byte/${now:%Y.%m.%d}/${now:%H%M%S}
  sweep:
    dir: exp/palindrome_byte/${now:%Y.%m.%d}/${now:%H%M%S}
    subdir: ${hydra.job.num}