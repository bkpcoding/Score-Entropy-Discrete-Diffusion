defaults:
  - _self_
  # - model: small

ngpus: 4  # Multi-GPU training
tokens: 259  # 256 bytes + PAD + BOS + EOS

training:
  batch_size: 64  # Smaller batch size for byte-level
  accum: 1
  n_iters: 75000
  snapshot_freq: 1000
  log_freq: 50
  eval_freq: 500
  snapshot_freq_for_preemption: 1000
  weight: standard
  snapshot_sampling: True
  ema: 0.999  # Faster EMA for quicker adaptation
  pretrain_checkpoint: null  # Path to pre-trained checkpoint
  disable_checkpoint_loading: false  # Set to true to start from scratch

data:
  train: byte_palindrome
  valid: byte_palindrome
  cache_dir: data

graph:
  type: uniform
  file: data
  report_all: False

noise:
  type: loglinear
  sigma_min: 1e-4
  sigma_max: 20

sampling:
  predictor: analytic
  steps: 64  # Fewer steps for faster sampling
  noise_removal: True

eval:
  batch_size: 32
  perplexity: True
  perplexity_batch_size: 16

optim:
  weight_decay: 0.01
  optimizer: AdamW
  lr: 3e-4
  beta1: 0.9
  beta2: 0.95
  eps: 1e-8
  warmup: 2000
  grad_clip: 1.
    # Muon optimizer settings
  use_muon: False  # Set to true to use Muon optimizer
  muon_lr: 0.002  # Learning rate for Muon (hidden weights)
  muon_momentum: 0.95
  muon_nesterov: True
  muon_ns_steps: 5
  adamw_lr: 3e-4  # Learning rate for AdamW (embeddings, heads, biases)



model:
  hidden_size: 768
  cond_dim: 96
  length: 32
  n_blocks: 12
  n_heads: 12
  scale_by_sigma: False
  dropout: 0.1

logging:
  # use_wandb: ${oc.env:USE_WANDB,false}  # Set USE_WANDB=true to enable
  use_wandb: false
  wandb_project: "palindrome-diffusion"
  wandb_entity: null  # Your wandb entity/username
  log_grad_norm: true
  log_param_norm: true
  log_learning_rate: true

hydra:
  run:
    dir: exp_local/palindrome_byte/${now:%Y.%m.%d}/${now:%H%M%S}
  sweep:
    dir: exp/palindrome_byte/${now:%Y.%m.%d}/${now:%H%M%S}
    subdir: ${hydra.job.num}